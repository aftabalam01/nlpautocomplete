{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Requirements\n",
    "!conda install -c conda-forge ptable -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/aftab.alam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "nltk.data.path.append('.')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <class 'str'>\n",
      "Number of letters: 3335477\n",
      "First 300 letters of the data\n",
      "-------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\\nWhen you meet someone special... you'll know. Your heart will beat more rapidly and you'll smile for no reason.\\nthey've decided its more fun if I don't.\\nSo Tired D; Played Lazer Tag & Ran A \""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "Last 300 letters of the data\n",
      "-------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"ust had one a few weeks back....hopefully we will be back soon! wish you the best yo\\nColombia is with an 'o'...“: We now ship to 4 countries in South America (fist pump). Please welcome Columbia to the Stunner Family”\\n#GutsiestMovesYouCanMake Giving a cat a bath.\\nCoffee after 5 was a TERRIBLE idea.\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH='.'\n",
    "data_file = 'en_US.twitter.txt'\n",
    "file = f'{DATA_PATH}/{data_file}'\n",
    "with open(file, \"r\") as f:\n",
    "    data = f.read()\n",
    "print(\"Data type:\", type(data))\n",
    "print(\"Number of letters:\", len(data))\n",
    "print(\"First 300 letters of the data\")\n",
    "print(\"-------\")\n",
    "display(data[0:300])\n",
    "print(\"-------\")\n",
    "\n",
    "print(\"Last 300 letters of the data\")\n",
    "print(\"-------\")\n",
    "display(data[-300:])\n",
    "print(\"-------\")\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS='$' # start of sentence token\n",
    "EOS='√' # end of senetence token\n",
    "UNK='ß' # unknown word token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocabulary using training data.\n",
    "# replace less occuring words with <unk>\n",
    "class vocabulary:\n",
    "    def __init__(self,tokens):\n",
    "        \"\"\"\n",
    "        list of train tokenized sentences \n",
    "        \"\"\"\n",
    "        self.tokens = tokens\n",
    "        self.char_counts = None\n",
    "    \n",
    "    def count_char(self):\n",
    "        \"\"\"\n",
    "        counts words and create a frequency dict\n",
    "        \"\"\"\n",
    "        counts = {}\n",
    "        for sentence in self.tokens:\n",
    "            for char in sentence:\n",
    "                if char in counts:\n",
    "                    counts[char] += 1\n",
    "                else:\n",
    "                    counts[char] = 1\n",
    "        self.char_counts = counts\n",
    "    def build_vocab(self,threshold):\n",
    "        \"\"\"\n",
    "        creates a closed vocab with words ocurring less than threshold replaced with <unk>\n",
    "        \"\"\"\n",
    "        closed_vocab = []\n",
    "        if not self.char_counts:\n",
    "            self.count_char()\n",
    "        for ch , cnt in self.char_counts.items():\n",
    "            if cnt >= threshold:\n",
    "                closed_vocab.append(ch)\n",
    "        self.vocab = closed_vocab\n",
    "        self.char2int = {ch: self.vocab.index(ch) for ch in self.vocab }\n",
    "        self.int2char = {self.vocab.index(ch): ch for ch in self.vocab }\n",
    "        return self.vocab              \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'd',\n",
       " ' ',\n",
       " 'h',\n",
       " 'u',\n",
       " 'm',\n",
       " 'r',\n",
       " 'e',\n",
       " 'l',\n",
       " 'p',\n",
       " 'y',\n",
       " 'g',\n",
       " 'o',\n",
       " 'b',\n",
       " 'c',\n",
       " 'w']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'd': 1,\n",
       " ' ': 2,\n",
       " 'h': 3,\n",
       " 'u': 4,\n",
       " 'm': 5,\n",
       " 'r': 6,\n",
       " 'e': 7,\n",
       " 'l': 8,\n",
       " 'p': 9,\n",
       " 'y': 10,\n",
       " 'g': 11,\n",
       " 'o': 12,\n",
       " 'b': 13,\n",
       " 'c': 14,\n",
       " 'w': 15}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = vocabulary([\"adada hum relply go bach\",\"how are you\"])\n",
    "vocab.build_vocab(1)\n",
    "vocab.char2int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare dataset\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self,data_file,):\n",
    "        with open(data_file, \"r\") as f:\n",
    "            self.data = f.read()\n",
    "        self._preprocess()\n",
    "    def _preprocess(self):\n",
    "        self.data = '\\n'.join([sentence.lower() for sentence in self.data.split('\\n')])\n",
    "    \n",
    "    def split_to_data(self,train=.8,dev=.1,test=.1):\n",
    "        \"\"\"\n",
    "        Split data by linebreak \"\\n\"\n",
    "\n",
    "        Args:\n",
    "            data: str\n",
    "\n",
    "        Returns:\n",
    "            A list of sentences\n",
    "        \"\"\"\n",
    "        sentences = nltk.tokenize.sent_tokenize(self.data)\n",
    "        print(f'Total Number of sentences: {len(sentences)}')\n",
    "        random.seed(87)\n",
    "        random.shuffle(sentences)\n",
    "\n",
    "        test_size = int(len(sentences) * test)\n",
    "        self.test_data = sentences[0:test_size]\n",
    "        train_dev_data = sentences[test_size:]\n",
    "        dev_size = int(len(sentences) * dev)\n",
    "        self.dev_data = train_dev_data[0:dev_size]\n",
    "        self.train_data = train_dev_data[dev_size:]\n",
    "    def vocab(self,threshold):\n",
    "        self.closed_vocab = set(vocabulary(tokens=self.train_data).build_vocab(threshold=threshold))\n",
    "    \n",
    "    def tokenize_sentences(self, data, n):\n",
    "        \"\"\"\n",
    "        Tokenize sentences into tokens (words)\n",
    "\n",
    "        Args:\n",
    "            sentences: List of strings\n",
    "\n",
    "        Returns:\n",
    "            List of lists of tokens\n",
    "        \"\"\"\n",
    "        ngram_tokenized_sentences = []\n",
    "        # Go through each sentence in train data \n",
    "        for sentence in data:\n",
    "            # Convert into a list of words\n",
    "            # ## add <s> and <e> tokens in data\n",
    "            sentence = SOS*(n-1) + sentence\n",
    "            tokenized=[]\n",
    "            for char_tuples in ngrams(sentence,n):\n",
    "                new_tuple=()\n",
    "                for ch in char_tuples:\n",
    "                    if ch != SOS and ch not in self.closed_vocab: # replace less frequent world with UNK\n",
    "                        ch = UNK\n",
    "                    new_tuple = new_tuple + (ch,)\n",
    "                tokenized.append(new_tuple)\n",
    "            # append the list of words to the list of lists\n",
    "            ngram_tokenized_sentences.append(tokenized)\n",
    "\n",
    "        return ngram_tokenized_sentences\n",
    "        \n",
    "    def get_tokenized_data(self,n_grams):\n",
    "        if not self.train_data:\n",
    "            self.split_to_data()\n",
    "\n",
    "        self.ngram_tokenized = self.tokenize_sentences(self.train_data, n_grams)\n",
    "        self.ngram_minus1_tokenized = self.tokenize_sentences(self.train_data, n_grams-1)\n",
    "        #self.ngram_minus2_tokenized = self.tokenize_sentences(self.train_data, n_grams-2)\n",
    "        self.test_tokenized = self.tokenize_sentences(self.test_data,n_grams)\n",
    "        self.dev_tokenized = self.tokenize_sentences(self.dev_data,n_grams)\n",
    "        return self.ngram_minus1_tokenized, self.ngram_tokenized, self.test_tokenized ,self.dev_tokenized\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 'b')\n",
      "('b', 'c')\n",
      "('c', 'd')\n",
      "('d', 'e')\n",
      "('e', 'r')\n",
      "('r', 's')\n"
     ]
    }
   ],
   "source": [
    "for tup in ngrams(\"abcders\",2):\n",
    "    print(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of sentences: 55661\n",
      "CPU times: user 5.28 s, sys: 51.5 ms, total: 5.34 s\n",
      "Wall time: 5.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "THESHOLD=2\n",
    "nGram =3\n",
    "dataset = DataSet(data_file=file)\n",
    "dataset.split_to_data()\n",
    "dataset.vocab(THESHOLD)\n",
    "closed_vocab = dataset.closed_vocab\n",
    "ngram_1minus_tokenized,ngram_tokenized, test_data,dev_data = dataset.get_tokenized_data(nGram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('$', 'h'),\n",
       "  ('h', 'a'),\n",
       "  ('a', 'h'),\n",
       "  ('h', 'a'),\n",
       "  ('a', 'h'),\n",
       "  ('h', 'a'),\n",
       "  ('a', 'a'),\n",
       "  ('a', '\\n'),\n",
       "  ('\\n', 'f'),\n",
       "  ('f', 'a'),\n",
       "  ('a', 'b'),\n",
       "  ('b', 'u'),\n",
       "  ('u', 'l'),\n",
       "  ('l', 'o'),\n",
       "  ('o', 'u'),\n",
       "  ('u', 's'),\n",
       "  ('s', ' '),\n",
       "  (' ', 'd'),\n",
       "  ('d', 'e'),\n",
       "  ('e', 's'),\n",
       "  ('s', 'i'),\n",
       "  ('i', 'g'),\n",
       "  ('g', 'n'),\n",
       "  ('n', ' '),\n",
       "  (' ', 't'),\n",
       "  ('t', 'i'),\n",
       "  ('i', 'p'),\n",
       "  ('p', ':'),\n",
       "  (':', ' '),\n",
       "  (' ', 'y'),\n",
       "  ('y', 'o'),\n",
       "  ('o', 'u'),\n",
       "  ('u', 'r'),\n",
       "  ('r', ' '),\n",
       "  (' ', 'h'),\n",
       "  ('h', 'o'),\n",
       "  ('o', 'm'),\n",
       "  ('m', 'e'),\n",
       "  ('e', ' '),\n",
       "  (' ', 'c'),\n",
       "  ('c', 'a'),\n",
       "  ('a', 'n'),\n",
       "  ('n', ' '),\n",
       "  (' ', 'h'),\n",
       "  ('h', 'a'),\n",
       "  ('a', 'v'),\n",
       "  ('v', 'e'),\n",
       "  ('e', ' '),\n",
       "  (' ', 't'),\n",
       "  ('t', 'h'),\n",
       "  ('h', 'e'),\n",
       "  ('e', ' '),\n",
       "  (' ', 'e'),\n",
       "  ('e', 's'),\n",
       "  ('s', 's'),\n",
       "  ('s', 'e'),\n",
       "  ('e', 'n'),\n",
       "  ('n', 'c'),\n",
       "  ('c', 'e'),\n",
       "  ('e', ' '),\n",
       "  (' ', 'o'),\n",
       "  ('o', 'f'),\n",
       "  ('f', ' '),\n",
       "  (' ', 'y'),\n",
       "  ('y', 'o'),\n",
       "  ('o', 'u'),\n",
       "  ('u', 'r'),\n",
       "  ('r', ' '),\n",
       "  (' ', 'f'),\n",
       "  ('f', 'a'),\n",
       "  ('a', 'v'),\n",
       "  ('v', 'o'),\n",
       "  ('o', 'r'),\n",
       "  ('r', 'i'),\n",
       "  ('i', 't'),\n",
       "  ('t', 'e'),\n",
       "  ('e', ' '),\n",
       "  (' ', 'l'),\n",
       "  ('l', 'o'),\n",
       "  ('o', 'o'),\n",
       "  ('o', 'k'),\n",
       "  ('k', '.')]]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[[('$', '$', 'g'),\n",
       "  ('$', 'g', 'o'),\n",
       "  ('g', 'o', '\\n'),\n",
       "  ('o', '\\n', 't'),\n",
       "  ('\\n', 't', 'h'),\n",
       "  ('t', 'h', 'a'),\n",
       "  ('h', 'a', 'n'),\n",
       "  ('a', 'n', 'k'),\n",
       "  ('n', 'k', ' '),\n",
       "  ('k', ' ', 'y'),\n",
       "  (' ', 'y', 'o'),\n",
       "  ('y', 'o', 'u'),\n",
       "  ('o', 'u', ' '),\n",
       "  ('u', ' ', 'a'),\n",
       "  (' ', 'a', 's'),\n",
       "  ('a', 's', 'h'),\n",
       "  ('s', 'h', 'l'),\n",
       "  ('h', 'l', 'e'),\n",
       "  ('l', 'e', 'y'),\n",
       "  ('e', 'y', '.')]]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_1minus_tokenized[0:1]\n",
    "dev_data[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split dataset in train and set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data are split into 44529 ngramtrain,5566 dev and 5566 test set\n",
      "Length vocab including UNK, SOS, and EOS is 221\n",
      "First training sample:\n",
      "[('$', '$', 'h'), ('$', 'h', 'a'), ('h', 'a', 'h'), ('a', 'h', 'a'), ('h', 'a', 'h'), ('a', 'h', 'a'), ('h', 'a', 'a'), ('a', 'a', '\\n'), ('a', '\\n', 'f'), ('\\n', 'f', 'a'), ('f', 'a', 'b'), ('a', 'b', 'u'), ('b', 'u', 'l'), ('u', 'l', 'o'), ('l', 'o', 'u'), ('o', 'u', 's'), ('u', 's', ' '), ('s', ' ', 'd'), (' ', 'd', 'e'), ('d', 'e', 's'), ('e', 's', 'i'), ('s', 'i', 'g'), ('i', 'g', 'n'), ('g', 'n', ' '), ('n', ' ', 't'), (' ', 't', 'i'), ('t', 'i', 'p'), ('i', 'p', ':'), ('p', ':', ' '), (':', ' ', 'y'), (' ', 'y', 'o'), ('y', 'o', 'u'), ('o', 'u', 'r'), ('u', 'r', ' '), ('r', ' ', 'h'), (' ', 'h', 'o'), ('h', 'o', 'm'), ('o', 'm', 'e'), ('m', 'e', ' '), ('e', ' ', 'c'), (' ', 'c', 'a'), ('c', 'a', 'n'), ('a', 'n', ' '), ('n', ' ', 'h'), (' ', 'h', 'a'), ('h', 'a', 'v'), ('a', 'v', 'e'), ('v', 'e', ' '), ('e', ' ', 't'), (' ', 't', 'h'), ('t', 'h', 'e'), ('h', 'e', ' '), ('e', ' ', 'e'), (' ', 'e', 's'), ('e', 's', 's'), ('s', 's', 'e'), ('s', 'e', 'n'), ('e', 'n', 'c'), ('n', 'c', 'e'), ('c', 'e', ' '), ('e', ' ', 'o'), (' ', 'o', 'f'), ('o', 'f', ' '), ('f', ' ', 'y'), (' ', 'y', 'o'), ('y', 'o', 'u'), ('o', 'u', 'r'), ('u', 'r', ' '), ('r', ' ', 'f'), (' ', 'f', 'a'), ('f', 'a', 'v'), ('a', 'v', 'o'), ('v', 'o', 'r'), ('o', 'r', 'i'), ('r', 'i', 't'), ('i', 't', 'e'), ('t', 'e', ' '), ('e', ' ', 'l'), (' ', 'l', 'o'), ('l', 'o', 'o'), ('o', 'o', 'k'), ('o', 'k', '.')]\n",
      "First test sample\n",
      "[('$', '$', 'i'), ('$', 'i', ' '), ('i', ' ', 'd'), (' ', 'd', 'i'), ('d', 'i', 'd'), ('i', 'd', 'n'), ('d', 'n', \"'\"), ('n', \"'\", 't'), (\"'\", 't', ' '), ('t', ' ', 's'), (' ', 's', 'e'), ('s', 'e', 'n'), ('e', 'n', 'd'), ('n', 'd', ' '), ('d', ' ', 'y'), (' ', 'y', 'u'), ('y', 'u', ' '), ('u', ' ', 'o'), (' ', 'o', 'f'), ('o', 'f', 'f'), ('f', 'f', '\\n'), ('f', '\\n', 'm'), ('\\n', 'm', 'y'), ('m', 'y', ' '), ('y', ' ', 'b'), (' ', 'b', 'r'), ('b', 'r', 'a'), ('r', 'a', 'n'), ('a', 'n', 'd'), ('n', 'd', ' '), ('d', ' ', 'i'), (' ', 'i', 's'), ('i', 's', ' '), ('s', ' ', 'g'), (' ', 'g', 'e'), ('g', 'e', 't'), ('e', 't', 't'), ('t', 't', 'i'), ('t', 'i', 'n'), ('i', 'n', 'g'), ('n', 'g', ' '), ('g', ' ', 'b'), (' ', 'b', 'i'), ('b', 'i', 'g'), ('i', 'g', 'g'), ('g', 'g', 'e'), ('g', 'e', 'r'), ('e', 'r', ' '), ('r', ' ', 'b'), (' ', 'b', 'y'), ('b', 'y', ' '), ('y', ' ', 't'), (' ', 't', 'h'), ('t', 'h', 'e'), ('h', 'e', ' '), ('e', ' ', 'd'), (' ', 'd', 'a'), ('d', 'a', 'y'), ('a', 'y', '!'), ('y', '!', '!'), ('!', '!', '!')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Data are split into {} ngramtrain,{} dev and {} test set\".format(\n",
    "    len(ngram_tokenized), len(dev_data), len(test_data)))\n",
    "print(f'Length vocab including UNK, SOS, and EOS is {len(closed_vocab)}')\n",
    "print(\"First training sample:\")\n",
    "print(ngram_tokenized[0])\n",
    "      \n",
    "print(\"First test sample\")\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closed vocabulary:\n",
      "['s', ' ', 'l', 'e', '.', 'a', 'r']\n"
     ]
    }
   ],
   "source": [
    "# test your code\n",
    "tokenized_sentences = ['sky is blue .',\n",
    "                       'leaves are green .',\n",
    "                       'roses are red .']\n",
    "vocab = vocabulary(tokenized_sentences)\n",
    "tmp_closed_vocab = vocab.build_vocab(threshold=2)\n",
    "print(f\"Closed vocabulary:\")\n",
    "print(tmp_closed_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "class Model:\n",
    "    def __init__(self,ngrams=2,n1gramsTrain=ngram_1minus_tokenized, ngramsTrain=ngram_tokenized,vocab=closed_vocab):\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.n_grams = ngrams\n",
    "        self.train_data_ngram = ngramsTrain\n",
    "        self.train_data_1ngram = n1gramsTrain\n",
    "\n",
    "    \n",
    "    def count_n_grams(self,data):\n",
    "        \"\"\"\n",
    "        Count words after ngrams in training data set\n",
    "        \"\"\"\n",
    "        if not data:\n",
    "            data = self.train_data_ngram\n",
    "        nGram_cnt ={}\n",
    "        for sentence in data:\n",
    "            for tuples in sentence:\n",
    "                if nGram_cnt.get(tuples,0):\n",
    "                    nGram_cnt[tuples] +=1\n",
    "                else:\n",
    "                    nGram_cnt[tuples] = 1\n",
    "        return nGram_cnt\n",
    "\n",
    "    \n",
    "    def calculate_ngram_probability(self, ngram, smoothing=1):\n",
    "        \"\"\"\n",
    "        calculate probabilities of given ngram\n",
    "        ngram  = w1,w2,..wn\n",
    "        n-1gram = w1,w2...wn-1\n",
    "        = (count(ngram) + k)/(count(n-1gram) + k*V)\n",
    "        where V is size of vocab\n",
    "        \"\"\"\n",
    "        count_ngram  = self.nGram_cnt.get(ngram,0)\n",
    "        nminus1_gram = ngram[:-1]\n",
    "        count_nminus1_gram = self.n1Gram_cnt.get(nminus1_gram,0)\n",
    "        probs = (count_ngram + smoothing)/(count_nminus1_gram + smoothing* self.vocab_size)\n",
    "       \n",
    "        return probs\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        calculate ngram and nminus gram counts\n",
    "        \"\"\"\n",
    "        self.nGram_cnt=self.count_n_grams(data=self.train_data_ngram)\n",
    "        self.n1Gram_cnt=self.count_n_grams(data=self.train_data_1ngram)\n",
    "    \n",
    "    def save(self,path,name,checkpoint):\n",
    "        model_path = f'{path}/{name}'\n",
    "        if not os.path.exists(model_path):\n",
    "            os.mkdir(model_path)\n",
    "        count_df = {'count_ngram':self.nGram_cnt, 'count_nminus1gram':self.n1Gram_cnt}\n",
    "        with open(f'{model_path}/{checkpoint}.pkl', 'wb') as fp:\n",
    "            pickle.dump(count_df, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    def load(self,path,name,checkpoint):\n",
    "        model_path = f'{path}/{name}'\n",
    "        with open(f'{model_path}/{checkpoint}.pkl', 'rb') as fp:\n",
    "            count_df = pickle.load(fp)\n",
    "            self.nGram_cnt = count_df['count_ngram']\n",
    "            self.n1Gram_cnt = count_df['count_nminus1gram']\n",
    "    \n",
    "    def predict_nextchar(self,ngram):\n",
    "        \"\"\"\n",
    "        Given a ngram find next words and their probabilities\n",
    "        \"\"\"\n",
    "        # n-1 history\n",
    "        next_hist = ngram[1:]\n",
    "        probs = {}\n",
    "        # list of ngrams\n",
    "        for ngram_tuple in self.nGram_cnt.keys():\n",
    "            hist = ngram_tuple[:-1]\n",
    "            char = ngram_tuple[-1]\n",
    "            if next_hist == hist:\n",
    "                prob = self.calculate_ngram_probability(ngram_tuple,1)\n",
    "                probs[char] = prob\n",
    "        if not probs: # return unknown word if model did not find any thing\n",
    "            probs = {UNK: 1/self.vocab_size}\n",
    "        \n",
    "        return probs\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ngram train set 3000 \n",
      "Ngram train set 1st sentence is  82 \n",
      "CPU times: user 83.9 ms, sys: 2.93 ms, total: 86.9 ms\n",
      "Wall time: 85.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# test code\n",
    "from collections import Counter\n",
    "model = Model(ngrams=nGram,n1gramsTrain=ngram_1minus_tokenized[0:3000],ngramsTrain=ngram_tokenized[0:3000],vocab=closed_vocab)\n",
    "print(f\"Ngram train set {len(model.train_data_ngram)} \")\n",
    "print(f\"Ngram train set 1st sentence is  {len(model.train_data_ngram[0])} \")\n",
    "model.train()\n",
    "model.save('.','char_model',1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3952451708766716"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'a': 0.05465004793863854,\n",
       " 'w': 0.11313518696069032,\n",
       " 'c': 0.07094918504314478,\n",
       " 's': 0.0488974113135187,\n",
       " 'g': 0.04410354745925216,\n",
       " 'm': 0.032598274209012464,\n",
       " 'l': 0.07957813998082454,\n",
       " 't': 0.04314477468839885,\n",
       " 'k': 0.029721955896452542,\n",
       " 'h': 0.07574304889741132,\n",
       " 'f': 0.02109300095877277,\n",
       " 'n': 0.030680728667305847,\n",
       " '*': 0.0019175455417066154,\n",
       " 'j': 0.02109300095877277,\n",
       " 'd': 0.052732502396931925,\n",
       " '&': 0.0028763183125599234,\n",
       " 'p': 0.012464046021093002,\n",
       " 'r': 0.02205177372962608,\n",
       " 'o': 0.009587727708533078,\n",
       " 'q': 0.003835091083413231,\n",
       " 'u': 0.006711409395973154,\n",
       " 'b': 0.012464046021093002,\n",
       " 'v': 0.003835091083413231,\n",
       " 'e': 0.009587727708533078,\n",
       " 'i': 0.003835091083413231,\n",
       " '❤': 0.0019175455417066154,\n",
       " '<': 0.0019175455417066154,\n",
       " 'y': 0.0028763183125599234,\n",
       " '(': 0.0019175455417066154}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(ngrams=nGram,n1gramsTrain=ngram_1minus_tokenized[0:3000],ngramsTrain=ngram_tokenized[0:3000],vocab=closed_vocab)\n",
    "model.load('.','char_model',1) \n",
    "ngram = ngram_tokenized[0][0]\n",
    "ngram =('$','i', ' ')\n",
    "model.calculate_ngram_probability(ngram,smoothing=1)\n",
    "model.predict_nextchar(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.nGram_cnt.get(('$','i', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dev_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity1(sentence):\n",
    "    \"\"\"\n",
    "    ngram tokenize sentence\n",
    "    \"\"\"\n",
    "    N = len(sentence)\n",
    "    #cross_entropy = − log2 p(x ̄; θ)/N\n",
    "    px = 1\n",
    "    if N:\n",
    "        for ngram in sentence:\n",
    "            p = model.calculate_ngram_probability(ngram,smoothing=1)\n",
    "            px *=p\n",
    "        cross_entropy = -1 * np.log2(px)/N\n",
    "    return 2**cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.149177030600175"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity1(dev_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.149177030600175"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def perplexity2(sentence):\n",
    "    N = len(sentence)\n",
    "    #PP = p ** (1/N)\n",
    "    px = 1\n",
    "    if N:\n",
    "        for ngram in sentence:\n",
    "            p = model.calculate_ngram_probability(ngram,smoothing=1)\n",
    "            px *= 1/p\n",
    "        return px ** (1/N)\n",
    "perplexity2(dev_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ngram train set 44529 \n",
      "Ngram train set 1st sentence is  82 \n",
      "CPU times: user 1.07 s, sys: 7.26 ms, total: 1.07 s\n",
      "Wall time: 1.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train using complete training data\n",
    "model = Model(ngrams=nGram,n1gramsTrain=ngram_1minus_tokenized,ngramsTrain=ngram_tokenized,vocab=closed_vocab)\n",
    "print(f\"Ngram train set {len(model.train_data_ngram)} \")\n",
    "print(f\"Ngram train set 1st sentence is  {len(model.train_data_ngram[0])} \")\n",
    "model.train()\n",
    "model.save('.','bigram_model',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221.00000000000003"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = []\n",
    "sent_g = ngrams(\"i would like to congratulate and for finishing their undergrad classes at wfu today.\",2)\n",
    "for s in sent_g:\n",
    "    sent = [*sent,s]\n",
    "perplexity2(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-159-10c767e6ef81>:12: RuntimeWarning: divide by zero encountered in log2\n",
      "  cross_entropy = -1 * np.log2(px)/N\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity of sent bad asa lol im at work listen to my cali music waitin to get off hows dancing going when u go shout me out in yo videos\n",
      "blue october is the most emotional band i've ever heard. is inf\n",
      "perplexity of sent bring back mr easy rider lol\n",
      "what could be better than prepping tax docs & viewing #epl on a saturday morning? is inf\n",
      "perplexity of sent amazing! is inf\n",
      "perplexity of sent hey, bri. is inf\n",
      "perplexity of sent hope you can find something else fun to entertain your guests. is inf\n",
      "perplexity of sent happy birthday\n",
      "my doc will also get the disks. is inf\n",
      "perplexity of sent together we make the best mexican ever! is inf\n",
      "perplexity of sent see u wednesday! is inf\n",
      "perplexity of sent looking for a new scent & like some options. is inf\n",
      "perplexity of sent you're amazing. is inf\n",
      "perplexity of sent whos in? is inf\n",
      "perplexity of sent gonna be tweeting from the #filmset tonight. is inf\n",
      "perplexity of sent but i'm at keuka college. is inf\n",
      "perplexity of sent full house for cripple of inishmaan! is inf\n",
      "perplexity of sent - if the boss says new jersey, it's new jersey\n",
      "i am talking to a number of nonprofits and child advocates in the area to maximize the impact of this effort. is inf\n",
      "perplexity of sent looking forward to sharing our experiences from the trenches. is inf\n",
      "perplexity of sent that sounds great, is it your film or are you seeing someone else's? is inf\n",
      "perplexity of sent i have had a headache for over 12 hours! is inf\n",
      "perplexity of sent it'll all get better in time <3\n",
      "okay i'll bite. is inf\n",
      "perplexity of sent beautiful day in tn looking at #beef #cattle and visiting with friends\n",
      "haha i tweeted that yesterday, and it was 2 am, and i thought it was like 8, so my phone sent me a reminder that it's tomtom\n",
      "thanks for the follow and look forward to working with u! is inf\n",
      "perplexity of sent im pretty sure i said the same thing awhile back\n",
      "taylor: in media ownership, must look at markets & see what's appealing, e.g. is inf\n",
      "perplexity of sent was that your door i knocked on?! is inf\n",
      "perplexity of sent books = paper(!) is inf\n",
      "perplexity of sent don't know if i can trust you. is inf\n",
      "perplexity of sent tix will also be sold at the door! is inf\n",
      "perplexity of sent big day for picksleague!!! is inf\n",
      "perplexity of sent make it the best yet! is inf\n",
      "perplexity of sent this freakin guy has been key. is inf\n",
      "perplexity of sent come see me!! is inf\n",
      "perplexity of sent sports (blog)\n",
      "bless fam....linkup! is inf\n",
      "perplexity of sent she once watched a bowl of salad fly off her counter when home alone...\n",
      "yes, i am, crazy week for me....how was yours? is inf\n",
      "perplexity of sent so obvs it's not that great. is inf\n",
      "perplexity of sent track that truck for affordable lobster rolls! is inf\n",
      "perplexity of sent things like that require a little more prep time for me though.. eloping in your future? is inf\n",
      "perplexity of sent ratatat, writing and studying the history of rock and roll. is inf\n",
      "perplexity of sent i just can't do both at once. is inf\n",
      "perplexity of sent i am definitely inspired. is inf\n",
      "perplexity of sent what a beautiful day\n",
      "baby you light up my world like nobody else , the way that you flip your hair gets me over whelmed <3\n",
      "i wish, but this isn't a business expense. is inf\n",
      "perplexity of sent yes! is inf\n",
      "perplexity of sent cc: ah! is inf\n",
      "perplexity of sent follow along at\n",
      "when you come from a life of not having to having too quickly that's where the problem comes into play. is inf\n",
      "perplexity of sent we've a lot to talk about at whatever conference we're both at next\n",
      "this town has way to many bitches. is inf\n",
      "perplexity of sent \\m/\n",
      "have you ever fucked a japanese girl? is inf\n",
      "perplexity of sent you donated 10,533 food items to the community food bank during food for fines week. is inf\n",
      "perplexity of sent (1/2) i'm so glad you guys are back! is inf\n",
      "perplexity of sent i can't wait! is inf\n",
      "perplexity of sent #sportingkc wins 3-0! is inf\n",
      "perplexity of sent way to show your stones brett and mac\n",
      "i cant stop listening to 11:11 its an amazing song! is inf\n",
      "perplexity of sent why am i not asleep?! is inf\n",
      "perplexity of sent :)\n",
      "thanks for playing the morning mindbender and congrats to who got the correct answer which is....\n",
      "sorry for the late reply, but that is just wrong! is inf\n",
      "perplexity of sent just a bad opinion issue. is inf\n",
      "perplexity of sent getting malaria shots tomorrow...\n",
      "i'm shocked at how great all the guys are on idol. is inf\n",
      "perplexity of sent :0)\n",
      "sometimes drinking a beer down by the river is all a man needs to clear his head. is inf\n",
      "perplexity of sent but you have 20 open tables?!? is inf\n",
      "perplexity of sent don't forget the playlist, no curse words! is inf\n",
      "perplexity of sent info to come. is inf\n",
      "perplexity of sent i'm slow. is inf\n",
      "perplexity of sent wish i could be in both! is inf\n",
      "perplexity of sent be polite and try everything on the table. is inf\n",
      "perplexity of sent thanks for the follow :) happy weekend! is inf\n",
      "perplexity of sent because when i do, no one can hear the pain......---\n",
      "ya i'm a little drunk on you and high on summertime\n",
      "great health tips for baby boomer guys on my health blog: www.itsaguythingblog.wordpress.com\n",
      "#ronpaul can win! is inf\n",
      "perplexity of sent so who's invited??!! is inf\n",
      "perplexity of sent new discounts! is inf\n",
      "perplexity of sent also found out - cousins in spain, burma/myanmar, germany, and hungary, too! is inf\n",
      "perplexity of sent over dumplings and sticky rice. is inf\n",
      "perplexity of sent so much to do! is inf\n",
      "perplexity of sent and it's awesome. is inf\n",
      "perplexity of sent get her what she really kraves a piece of #mikelpatrik, origs & reproductions for any budget! is inf\n",
      "perplexity of sent finishing dinner for the boys, will come when done\n",
      "fight me\n",
      "aww, you're crying because of that song? is inf\n",
      "perplexity of sent i haven't cried for two days. is inf\n",
      "perplexity of sent join team germain for the susan g. komen race for the cure on may 15th! is inf\n",
      "perplexity of sent i agree. is inf\n",
      "perplexity of sent have a happy and safe 4th of july!! is inf\n",
      "perplexity of sent thanks to everyone who came out & most esp. is inf\n",
      "perplexity of sent the banana pudding milkshake from chick-fil-a is absolutely incredible!! is inf\n",
      "perplexity of sent the banana pudding milkshake from chick-fil-a is absolutely incredible!! is inf\n",
      "perplexity of sent coming tomorrow bni? is inf\n",
      "perplexity of sent there is no february 30th <3\n",
      "i want to work in the mail room. is inf\n",
      "perplexity of sent thanks for letting me know that. is inf\n",
      "perplexity of sent haven't seen you for awhile. is inf\n",
      "perplexity of sent hey thanks! is inf\n",
      "perplexity of sent \"if i'm going to watch a movie here, do i need to check it out first?\" is inf\n",
      "perplexity of sent the met is doing janacek this week and so are we. is inf\n",
      "perplexity of sent #moodle 2.2. mobile app will let you download course content to your mobile device to read/view\n",
      "greg bailey formally announces his presidential campaign for 2012.\n",
      "we are now following!!! is inf\n",
      "perplexity of sent getting a pedi 💁ß\n",
      "naked & famous concert\n",
      "thanks for the follow corey! is inf\n",
      "perplexity of sent haha i knew someone would ride by and be like \"what in the??\" is inf\n",
      "perplexity of sent i really hope my visit to the emergency dentist today doesn't interfere with my drinking plans for tonight and the rest of the weekend. is inf\n",
      "perplexity of sent look at the stars. is inf\n",
      "perplexity of sent excited to be presenting today with , and at\n",
      "ptsd for sure. is inf\n",
      "perplexity of sent to get what you want, design a business that gives a large number of people what they want. is inf\n",
      "perplexity of sent got a couple of notes done...then tutoring and more after class.....\n",
      "i just want my daddy that's all. is inf\n",
      "perplexity of sent (and not on pinterest) does that count? is inf\n",
      "perplexity of sent class in the morning babe :(. is inf\n",
      "perplexity of sent we are so happy to be there every month! is inf\n",
      "perplexity of sent i need to watch tsc too, tonight. is inf\n",
      "perplexity of sent maybe we'll get new uni's for every game like oregon does! is inf\n",
      "perplexity of sent ~\n",
      "my dad made a 2 hour drive in 40 mins on a bike. is inf\n",
      "perplexity of sent all my cats stuff has been blown to oz. is inf\n",
      "perplexity of sent <3 that show\n",
      "please share info re personal workflows w/me! is inf\n",
      "perplexity of sent rondo at the buzzer! is inf\n",
      "perplexity of sent yeah safe house looks good to - this star wars ish is ridiculous line around the block\n",
      "what game are you guys going to be covering? is inf\n",
      "perplexity of sent enjoy your trip! is inf\n",
      "perplexity of sent *my favorite part*\n",
      "no one says no to big ben! is inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity of sent #adele's voice. is inf\n",
      "perplexity of sent i'm wondering whether banning mobiles in the classroom is missing the point - use technology to support learning\n",
      "long island\n",
      "what about your fans from washington? is inf\n",
      "perplexity of sent you are most welcome! is inf\n"
     ]
    }
   ],
   "source": [
    "# dev data perplexity\n",
    "dev_perflexity =[]\n",
    "#dev_data[:1]\n",
    "for sentence in dev_data:\n",
    "    pp = perplexity1(sentence) \n",
    "    if not pp == float('inf'):\n",
    "        sent = ''.join(wrd[-1] for wrd in sentence)\n",
    "        dev_perflexity = [*dev_perflexity,[pp,sent]]\n",
    "    else: print(f\"perplexity of sent {sent} is {pp}\")\n",
    "    # dev data perplexity\n",
    "test_perflexity =[]\n",
    "#test_data[:1]\n",
    "for sentence in test_data:\n",
    "    pp = perplexity1(sentence) \n",
    "    if not pp == float('inf'):\n",
    "        sent = ''.join(wrd[-1] for wrd in sentence)\n",
    "        test_perflexity = [*test_perflexity,[pp,sent]]\n",
    "    else: print(f\"perplexity of sent {sent} is {pp}\")\n",
    "# dev data perplexity\n",
    "train_perflexity =[]\n",
    "#ngram_tokenized[:1]\n",
    "for sentence in ngram_tokenized:\n",
    "    pp = perplexity1(sentence) \n",
    "    if not pp == float('inf'):\n",
    "        sent = ''.join(wrd[-1] for wrd in sentence)\n",
    "        train_perflexity = [*train_perflexity,[pp,sent]]\n",
    "    else: print(f\"perplexity of sent {sent} is {pp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean perflexity on train data: 10.898957192550023\n",
      "mean perflexity on dev data: 10.889734799608162\n",
      "mean perflexity on test data: 10.912576828796414\n"
     ]
    }
   ],
   "source": [
    "print(f\"mean perflexity on train data: {np.asarray(np.array(train_perflexity)[:,0],dtype='float64').mean()}\")\n",
    "\n",
    "print(f\"mean perflexity on dev data: {np.asarray(np.array(dev_perflexity)[:,0],dtype='float64').mean()}\")\n",
    "print(f\"mean perflexity on test data: {np.asarray(np.array(test_perflexity)[:,0],dtype='float64').mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' ', 't'), ('t', 'w'), ('w', 'i'), ('i', 't'), ('t', 't'), ('t', 'e'), ('e', 'r'), ('r', ' '), (' ', 'f'), ('f', 'e'), ('e', 'e'), ('e', 'd'), ('d', ' '), (' ', 'i'), ('i', 's'), ('s', ' '), (' ', 'f'), ('f', 'u'), ('u', 'l'), ('l', 'l'), ('l', ' '), (' ', 'o'), ('o', 'f'), ('f', ' '), (' ', 's'), ('s', 'c'), ('c', 'a'), ('a', 'r'), ('r', 'y'), ('y', ' '), (' ', 't'), ('t', 'o'), ('o', 'r'), ('r', 'n'), ('n', 'a'), ('a', 'd'), ('d', 'o'), ('o', 'e'), ('e', 's'), ('s', '.'), ('.', '√')]\n",
      " en   eto   tn to lltuot one t ue n u  \n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "test_sentence = test_data[1][3:]\n",
    "print(test_sentence)\n",
    "s_prob = []\n",
    "index=[]\n",
    "N=0\n",
    "for tuples in test_sentence:\n",
    "    prob  = model.predict_nextchar(tuples) # get char with all probs\n",
    "    pred = max(prob.items(), key=operator.itemgetter(1))[0]\n",
    "    if pred == '√':\n",
    "        break\n",
    "    index = index + [pred]\n",
    "    s_prob += [prob]\n",
    "    N +=1\n",
    "print(''.join(word for word in index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted charaters are:[' ', '.', '\\n'] and selected char is: . and sentence is: i am hap\n",
      "Predicted charaters are:['.', '\"', 'c'] and selected char is: c and sentence is: i am happ\n",
      "Predicted charaters are:['o', 'a', '.'] and selected char is: o and sentence is: i am happy\n",
      "Predicted charaters are:['m', 'n', 'u'] and selected char is: n and sentence is: i am happy.\n",
      "Predicted charaters are:[' ', 'e', 's'] and selected char is:   and sentence is: i am happy.c\n",
      "Predicted charaters are:['t', 'a', 'i'] and selected char is: a and sentence is: i am happy.co\n",
      "Predicted charaters are:[' ', 'n', 'l'] and selected char is:   and sentence is: i am happy.con\n",
      "Predicted charaters are:['b', 's', 'g'] and selected char is: b and sentence is: i am happy.con \n",
      "Predicted charaters are:['e', 'u', 'a'] and selected char is: e and sentence is: i am happy.con a\n",
      "Predicted charaters are:[' ', 'e', 'r'] and selected char is:   and sentence is: i am happy.con a \n",
      "Predicted charaters are:['t', 'a', 'i'] and selected char is: t and sentence is: i am happy.con a b\n",
      "Predicted charaters are:['h', 'o', 'i'] and selected char is: o and sentence is: i am happy.con a be\n",
      "Predicted charaters are:[' ', 'n', 'd'] and selected char is:   and sentence is: i am happy.con a be \n",
      "Predicted charaters are:['t', 's', 'b'] and selected char is: t and sentence is: i am happy.con a be t\n",
      "Predicted charaters are:['h', 'o', 'i'] and selected char is: h and sentence is: i am happy.con a be to\n",
      "Predicted charaters are:['e', 'a', 'i'] and selected char is: i and sentence is: i am happy.con a be to \n",
      "Predicted charaters are:['n', 's', 'l'] and selected char is: s and sentence is: i am happy.con a be to t\n",
      "Predicted charaters are:[' ', 't', 'h'] and selected char is: h and sentence is: i am happy.con a be to th\n",
      "Predicted charaters are:['o', 'e', 'i'] and selected char is: i and sentence is: i am happy.con a be to thi\n",
      "Predicted charaters are:['n', 's', 'l'] and selected char is: s and sentence is: i am happy.con a be to this\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "seed = 'i am happy'\n",
    "ngram_t = ngrams(seed,3)\n",
    "tokens = [t for t in ngram_t]\n",
    "# Predict next 3 char and choose 1\n",
    "for i in range(20):\n",
    "    prob  = model.predict_nextchar(tokens[-1]) # pass last ngram\n",
    "    chars = sorted(prob, key=prob.get, reverse=True)[:3]\n",
    "    ch = random.choice(chars)\n",
    "    if ch == UNK:\n",
    "        ch = random.choice(chars)\n",
    "    print(f\"Predicted charaters are:{chars} and selected char is: {ch} and sentence is: {''.join(ch[0] for ch in tokens)}\")\n",
    "    new_gram = tokens[-1][1:]+(ch,)\n",
    "    tokens = [ *tokens,new_gram]\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('y', '\\n', 'a')"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "('p', 'y', '\\n')[1:] + ('a',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-129-e2a96e5f2a70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mprettytable\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPrettyTable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Word'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Screen Name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Hashtag'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhashtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPrettyTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'words' is not defined"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "for label, data in (('Word', words),('Screen Name', screen_names),('Hashtag', hashtags)):\n",
    "\n",
    "    pt = PrettyTable(field_names=[label, 'Count'])    \n",
    "    c = Counter(data)    \n",
    "    [ pt.add_row(kv) for kv in c.most_common()[:10] ]   \n",
    "    pt.align[label], pt.align['Count'] = 'l', 'r'\n",
    "    # Set column alignmentprint(pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - ptable\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ptable-0.9.2               |             py_0          22 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:          22 KB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  ptable             conda-forge/noarch::ptable-0.9.2-py_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "ptable-0.9.2         | 22 KB     | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
