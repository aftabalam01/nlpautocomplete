{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Requirements\n",
    "!conda install -c conda-forge ptable -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/aftab.alam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "nltk.data.path.append('.')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <class 'str'>\n",
      "Number of letters: 3335477\n",
      "First 300 letters of the data\n",
      "-------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\\nWhen you meet someone special... you'll know. Your heart will beat more rapidly and you'll smile for no reason.\\nthey've decided its more fun if I don't.\\nSo Tired D; Played Lazer Tag & Ran A \""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "Last 300 letters of the data\n",
      "-------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"ust had one a few weeks back....hopefully we will be back soon! wish you the best yo\\nColombia is with an 'o'...“: We now ship to 4 countries in South America (fist pump). Please welcome Columbia to the Stunner Family”\\n#GutsiestMovesYouCanMake Giving a cat a bath.\\nCoffee after 5 was a TERRIBLE idea.\\n\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH='.'\n",
    "data_file = 'en_US.twitter.txt'\n",
    "file = f'{DATA_PATH}/{data_file}'\n",
    "with open(file, \"r\") as f:\n",
    "    data = f.read()\n",
    "print(\"Data type:\", type(data))\n",
    "print(\"Number of letters:\", len(data))\n",
    "print(\"First 300 letters of the data\")\n",
    "print(\"-------\")\n",
    "display(data[0:300])\n",
    "print(\"-------\")\n",
    "\n",
    "print(\"Last 300 letters of the data\")\n",
    "print(\"-------\")\n",
    "display(data[-300:])\n",
    "print(\"-------\")\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS='<s>' # start of sentence token\n",
    "EOS='<e>' # end of senetence token\n",
    "UNK='<unk>' # unknown word token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocabulary using training data.\n",
    "# replace less occuring words with <unk>\n",
    "class vocabulary:\n",
    "    def __init__(self,tokens):\n",
    "        \"\"\"\n",
    "        list of train tokenized sentences \n",
    "        \"\"\"\n",
    "        self.tokens = tokens\n",
    "        self.word_counts = None\n",
    "    \n",
    "    def count_words(self):\n",
    "        \"\"\"\n",
    "        counts words and create a frequency dict\n",
    "        \"\"\"\n",
    "        counts = {}\n",
    "        for sentence in self.tokens:\n",
    "            for word in nltk.word_tokenize(sentence):\n",
    "                if word in counts:\n",
    "                    counts[word] += 1\n",
    "                else:\n",
    "                    counts[word] = 1\n",
    "        self.word_counts = counts\n",
    "    def build_vocab(self,threshold):\n",
    "        \"\"\"\n",
    "        creates a closed vocab with words ocurring less than threshold replaced with <unk>\n",
    "        \"\"\"\n",
    "        closed_vocab = []\n",
    "        if not self.word_counts:\n",
    "            self.count_words()\n",
    "        for word , cnt in self.word_counts.items():\n",
    "            if cnt >= threshold:\n",
    "                closed_vocab.append(word)\n",
    "        self.vocab = closed_vocab\n",
    "        return self.vocab              \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare dataset\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self,data_file,):\n",
    "        with open(data_file, \"r\") as f:\n",
    "            self.data = f.read()\n",
    "        self._preprocess()\n",
    "    def _preprocess(self):\n",
    "        self.data = '\\n'.join([sentence.lower() for sentence in self.data.split('\\n')])\n",
    "    \n",
    "    def split_to_data(self,train=.8,dev=.1,test=.1):\n",
    "        \"\"\"\n",
    "        Split data by linebreak \"\\n\"\n",
    "\n",
    "        Args:\n",
    "            data: str\n",
    "\n",
    "        Returns:\n",
    "            A list of sentences\n",
    "        \"\"\"\n",
    "        sentences = nltk.tokenize.sent_tokenize(self.data)\n",
    "        print(f'Total Number of sentences: {len(sentences)}')\n",
    "        random.seed(87)\n",
    "        random.shuffle(sentences)\n",
    "\n",
    "        test_size = int(len(sentences) * test)\n",
    "        self.test_data = sentences[0:test_size]\n",
    "        train_dev_data = sentences[test_size:]\n",
    "        dev_size = int(len(sentences) * dev)\n",
    "        self.dev_data = train_dev_data[0:dev_size]\n",
    "        self.train_data = train_dev_data[dev_size:]\n",
    "    def vocab(self,threshold):\n",
    "        self.closed_vocab = set(vocabulary(tokens=self.train_data).build_vocab(threshold=threshold)+[EOS] + [UNK])\n",
    "    \n",
    "    def tokenize_sentences(self, data, n):\n",
    "        \"\"\"\n",
    "        Tokenize sentences into tokens (words)\n",
    "\n",
    "        Args:\n",
    "            sentences: List of strings\n",
    "\n",
    "        Returns:\n",
    "            List of lists of tokens\n",
    "        \"\"\"\n",
    "        ngram_tokenized_sentences = []\n",
    "        # Go through each sentence in train data \n",
    "        for sentence in data:\n",
    "            # Convert into a list of words\n",
    "            # ## add <s> and <e> tokens in data\n",
    "            tokens = [SOS]*(n-1) + nltk.word_tokenize(sentence) + [EOS]\n",
    "            tokenized=[]\n",
    "            for word_tuples in ngrams(tokens,n):\n",
    "                new_tuple=()\n",
    "                for word in word_tuples:\n",
    "                    if word != SOS and word not in self.closed_vocab: # replace less frequent world with UNK\n",
    "                        word = UNK\n",
    "                    new_tuple = new_tuple + (word,)\n",
    "                tokenized.append(new_tuple)\n",
    "            # append the list of words to the list of lists\n",
    "            ngram_tokenized_sentences.append(tokenized)\n",
    "\n",
    "        return ngram_tokenized_sentences\n",
    "        \n",
    "    def get_tokenized_data(self,n_grams):\n",
    "        if not self.train_data:\n",
    "            self.split_to_data()\n",
    "\n",
    "        self.ngram_tokenized = self.tokenize_sentences(self.train_data, n_grams)\n",
    "        self.ngram_minus1_tokenized = self.tokenize_sentences(self.train_data, n_grams-1)\n",
    "        #self.ngram_minus2_tokenized = self.tokenize_sentences(self.train_data, n_grams-2)\n",
    "        self.test_tokenized = self.tokenize_sentences(self.test_data,n_grams)\n",
    "        self.dev_tokenized = self.tokenize_sentences(self.dev_data,n_grams)\n",
    "        return self.ngram_minus1_tokenized, self.ngram_tokenized, self.test_tokenized ,self.dev_tokenized\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "(20,)\n",
      "(30,)\n",
      "(40,)\n",
      "(100,)\n",
      "(10,)\n",
      "(20,)\n",
      "(30,)\n",
      "(40,)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "for word in ngrams([10,20,30,40,100],1):\n",
    "    print(word)\n",
    "for word in ngrams([10,20,30,40,100],0):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of sentences: 55661\n",
      "CPU times: user 16.4 s, sys: 44.4 ms, total: 16.5 s\n",
      "Wall time: 16.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "THESHOLD=2\n",
    "nGram =2\n",
    "dataset = DataSet(data_file=file)\n",
    "dataset.split_to_data()\n",
    "dataset.vocab(THESHOLD)\n",
    "closed_vocab = dataset.closed_vocab\n",
    "ngram_1minus_tokenized,ngram_tokenized, test_data,dev_data = dataset.get_tokenized_data(nGram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('hahahaa',),\n",
       "  ('fabulous',),\n",
       "  ('design',),\n",
       "  ('tip',),\n",
       "  (':',),\n",
       "  ('your',),\n",
       "  ('home',),\n",
       "  ('can',),\n",
       "  ('have',),\n",
       "  ('the',),\n",
       "  ('essence',),\n",
       "  ('of',),\n",
       "  ('your',),\n",
       "  ('favorite',),\n",
       "  ('look',),\n",
       "  ('.',),\n",
       "  ('<e>',)]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[[('<s>', 'go'),\n",
       "  ('go', 'thank'),\n",
       "  ('thank', 'you'),\n",
       "  ('you', 'ashley'),\n",
       "  ('ashley', '.'),\n",
       "  ('.', '<e>')]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_1minus_tokenized[0:1]\n",
    "dev_data[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split dataset in train and set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data are split into 44529 ngramtrain,5566 dev and 5566 test set\n",
      "Length vocab including UNK, SOS, and EOS is 14785\n",
      "First training sample:\n",
      "[('<s>', 'hahahaa'), ('hahahaa', 'fabulous'), ('fabulous', 'design'), ('design', 'tip'), ('tip', ':'), (':', 'your'), ('your', 'home'), ('home', 'can'), ('can', 'have'), ('have', 'the'), ('the', 'essence'), ('essence', 'of'), ('of', 'your'), ('your', 'favorite'), ('favorite', 'look'), ('look', '.'), ('.', '<e>')]\n",
      "First test sample\n",
      "[('<s>', 'i'), ('i', 'did'), ('did', \"n't\"), (\"n't\", 'send'), ('send', 'yu'), ('yu', 'off'), ('off', 'my'), ('my', 'brand'), ('brand', 'is'), ('is', 'getting'), ('getting', 'bigger'), ('bigger', 'by'), ('by', 'the'), ('the', 'day'), ('day', '!'), ('!', '!'), ('!', '!'), ('!', '<e>')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Data are split into {} ngramtrain,{} dev and {} test set\".format(\n",
    "    len(ngram_tokenized), len(dev_data), len(test_data)))\n",
    "print(f'Length vocab including UNK, SOS, and EOS is {len(closed_vocab)}')\n",
    "print(\"First training sample:\")\n",
    "print(ngram_tokenized[0])\n",
    "      \n",
    "print(\"First test sample\")\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closed vocabulary:\n",
      "['.', 'are']\n"
     ]
    }
   ],
   "source": [
    "# test your code\n",
    "tokenized_sentences = ['sky is blue .',\n",
    "                       'leaves are green .',\n",
    "                       'roses are red .']\n",
    "vocab = vocabulary(tokenized_sentences)\n",
    "tmp_closed_vocab = vocab.build_vocab(threshold=2)\n",
    "print(f\"Closed vocabulary:\")\n",
    "print(tmp_closed_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "class Model:\n",
    "    def __init__(self,ngrams=2,n1gramsTrain=ngram_1minus_tokenized, ngramsTrain=ngram_tokenized,vocab=closed_vocab):\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.n_grams = ngrams\n",
    "        self.train_data_ngram = ngramsTrain\n",
    "        self.train_data_1ngram = n1gramsTrain\n",
    "\n",
    "    \n",
    "    def count_n_grams(self,data):\n",
    "        \"\"\"\n",
    "        Count words after ngrams in training data set\n",
    "        \"\"\"\n",
    "        if not data:\n",
    "            data = self.train_data_ngram\n",
    "        nGram_cnt ={}\n",
    "        for sentence in data:\n",
    "            for tuples in sentence:\n",
    "                if nGram_cnt.get(tuples,0):\n",
    "                    nGram_cnt[tuples] +=1\n",
    "                else:\n",
    "                    nGram_cnt[tuples] = 1\n",
    "        return nGram_cnt\n",
    "\n",
    "    \n",
    "    def calculate_ngram_probability(self, ngram, smoothing=1):\n",
    "        \"\"\"\n",
    "        calculate probabilities of given ngram\n",
    "        ngram  = w1,w2,..wn\n",
    "        n-1gram = w1,w2...wn-1\n",
    "        = (count(ngram) + k)/(count(n-1gram) + k*V)\n",
    "        where V is size of vocab\n",
    "        \"\"\"\n",
    "        count_ngram  = self.nGram_cnt.get(ngram,0)\n",
    "        nminus1_gram = ngram[:-1]\n",
    "        count_nminus1_gram = self.n1Gram_cnt.get(nminus1_gram,0)\n",
    "        probs = (count_ngram + smoothing)/(count_nminus1_gram + smoothing* self.vocab_size)\n",
    "       \n",
    "        return probs\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        calculate ngram and nminus gram counts\n",
    "        \"\"\"\n",
    "        self.nGram_cnt=self.count_n_grams(data=self.train_data_ngram)\n",
    "        self.n1Gram_cnt=self.count_n_grams(data=self.train_data_1ngram)\n",
    "    \n",
    "    def save(self,path,name,checkpoint):\n",
    "        model_path = f'{path}/{name}'\n",
    "        if not os.path.exists(model_path):\n",
    "            os.mkdir(model_path)\n",
    "        count_df = {'count_ngram':self.nGram_cnt, 'count_nminus1gram':self.n1Gram_cnt}\n",
    "        with open(f'{model_path}/{checkpoint}.pkl', 'wb') as fp:\n",
    "            pickle.dump(count_df, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    def load(self,path,name,checkpoint):\n",
    "        model_path = f'{path}/{name}'\n",
    "        with open(f'{model_path}/{checkpoint}.pkl', 'rb') as fp:\n",
    "            count_df = pickle.load(fp)\n",
    "            self.nGram_cnt = count_df['count_ngram']\n",
    "            self.n1Gram_cnt = count_df['count_nminus1gram']\n",
    "    \n",
    "    def predict_nextword(self,ngram):\n",
    "        \"\"\"\n",
    "        Given a ngram find next words and their probabilities\n",
    "        \"\"\"\n",
    "        # n-1 history\n",
    "        next_hist = ngram[1:]\n",
    "        probs = {}\n",
    "        # list of ngrams\n",
    "        for ngram_tuple in self.nGram_cnt.keys():\n",
    "            hist = ngram_tuple[:-1]\n",
    "            word = ngram_tuple[-1]\n",
    "            if next_hist == hist:\n",
    "                prob = self.calculate_ngram_probability(ngram_tuple,1)\n",
    "                probs[word] = prob\n",
    "        if not probs: # return unknown word if model did not find any thing\n",
    "            probs = {UNK: 1/self.vocab_size}\n",
    "        \n",
    "        return probs\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ngram train set 30 \n",
      "Ngram train set 1st sentence is  17 \n",
      "CPU times: user 1.51 ms, sys: 1.87 ms, total: 3.37 ms\n",
      "Wall time: 1.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# test code\n",
    "from collections import Counter\n",
    "model = Model(ngrams=nGram,n1gramsTrain=ngram_1minus_tokenized[0:30],ngramsTrain=ngram_tokenized[0:30],vocab=closed_vocab)\n",
    "print(f\"Ngram train set {len(model.train_data_ngram)} \")\n",
    "print(f\"Ngram train set 1st sentence is  {len(model.train_data_ngram[0])} \")\n",
    "model.train()\n",
    "model.save('.','test_model',1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.763611768684478e-05"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'hahahaa': 0.00013527223537368956,\n",
       " 'everyone': 0.00013527223537368956,\n",
       " 'power': 0.00013527223537368956,\n",
       " 'muah': 0.00013527223537368956,\n",
       " 'but': 0.00013527223537368956,\n",
       " '~': 0.00013527223537368956,\n",
       " 'hm': 0.00013527223537368956,\n",
       " 'and': 0.00013527223537368956,\n",
       " '<unk>': 0.00013527223537368956,\n",
       " 'your': 0.00013527223537368956,\n",
       " 'grr': 0.00013527223537368956,\n",
       " 'hope': 0.00020290835306053433,\n",
       " 'ouch': 0.00020290835306053433,\n",
       " 'although': 0.00013527223537368956,\n",
       " 'he': 0.00013527223537368956,\n",
       " 'will': 0.00013527223537368956,\n",
       " 'please': 0.00013527223537368956,\n",
       " ':': 0.00013527223537368956,\n",
       " 'i': 0.0002705444707473791,\n",
       " 'ah': 0.00013527223537368956,\n",
       " 'bring': 0.00013527223537368956,\n",
       " 'yes': 0.00013527223537368956,\n",
       " 'going': 0.00013527223537368956,\n",
       " 'haha': 0.00013527223537368956,\n",
       " 'if': 0.00013527223537368956,\n",
       " 'amazing': 0.00013527223537368956}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(ngrams=nGram,n1gramsTrain=ngram_1minus_tokenized[0:30],ngramsTrain=ngram_tokenized[0:30],vocab=closed_vocab)\n",
    "model.load('.','test_model',1) \n",
    "ngram = ngram_tokenized[0][0]\n",
    "ngram =('<s>','<s>')\n",
    "model.calculate_ngram_probability(ngram,smoothing=1)\n",
    "model.predict_nextword(ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dev_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity1(sentence):\n",
    "    \"\"\"\n",
    "    ngram tokenize sentence\n",
    "    \"\"\"\n",
    "    N = len(sentence)\n",
    "    #cross_entropy = − log2 p(x ̄; θ)/N\n",
    "    px = 1\n",
    "    for ngram in sentence:\n",
    "        px *= model.calculate_ngram_probability(ngram,smoothing=1)\n",
    "    cross_entropy = -1 * np.log2(px)/N\n",
    "    return 2**cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.568827010861229\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12151.333381541588"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity1(dev_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12151.33338154158"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def perplexity2(sentence):\n",
    "    N = len(sentence)\n",
    "    #PP = p ** (1/N)\n",
    "    px = 1\n",
    "    for ngram in sentence:\n",
    "        p = model.calculate_ngram_probability(ngram,smoothing=1)\n",
    "        px *= 1/p\n",
    "    return px ** (1/N)\n",
    "perplexity2(dev_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ngram train set 44529 \n",
      "Ngram train set 1st sentence is  17 \n",
      "CPU times: user 461 ms, sys: 176 ms, total: 637 ms\n",
      "Wall time: 2.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train using complete training data\n",
    "model = Model(ngrams=nGram,n1gramsTrain=ngram_1minus_tokenized,ngramsTrain=ngram_tokenized,vocab=closed_vocab)\n",
    "print(f\"Ngram train set {len(model.train_data_ngram)} \")\n",
    "print(f\"Ngram train set 1st sentence is  {len(model.train_data_ngram[0])} \")\n",
    "model.train()\n",
    "model.save('.','bigram_model',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('<s>', 'go'),\n",
       "  ('go', 'thank'),\n",
       "  ('thank', 'you'),\n",
       "  ('you', 'ashley'),\n",
       "  ('ashley', '.'),\n",
       "  ('.', '<e>')]]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[[('<s>', 'i'),\n",
       "  ('i', 'did'),\n",
       "  ('did', \"n't\"),\n",
       "  (\"n't\", 'send'),\n",
       "  ('send', 'yu'),\n",
       "  ('yu', 'off'),\n",
       "  ('off', 'my'),\n",
       "  ('my', 'brand'),\n",
       "  ('brand', 'is'),\n",
       "  ('is', 'getting'),\n",
       "  ('getting', 'bigger'),\n",
       "  ('bigger', 'by'),\n",
       "  ('by', 'the'),\n",
       "  ('the', 'day'),\n",
       "  ('day', '!'),\n",
       "  ('!', '!'),\n",
       "  ('!', '!'),\n",
       "  ('!', '<e>')]]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[[('<s>', 'hahahaa'),\n",
       "  ('hahahaa', 'fabulous'),\n",
       "  ('fabulous', 'design'),\n",
       "  ('design', 'tip'),\n",
       "  ('tip', ':'),\n",
       "  (':', 'your'),\n",
       "  ('your', 'home'),\n",
       "  ('home', 'can'),\n",
       "  ('can', 'have'),\n",
       "  ('have', 'the'),\n",
       "  ('the', 'essence'),\n",
       "  ('essence', 'of'),\n",
       "  ('of', 'your'),\n",
       "  ('your', 'favorite'),\n",
       "  ('favorite', 'look'),\n",
       "  ('look', '.'),\n",
       "  ('.', '<e>')]]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dev data perplexity\n",
    "dev_perflexity =[]\n",
    "dev_data[:1]\n",
    "for sentence in dev_data:\n",
    "    pp = perplexity2(sentence) \n",
    "    sent = ' '.join(wrd[-1] for wrd in sentence)\n",
    "    dev_perflexity = [*dev_perflexity,[pp,sent]]\n",
    "\n",
    "    # dev data perplexity\n",
    "test_perflexity =[]\n",
    "test_data[:1]\n",
    "for sentence in test_data:\n",
    "    pp = perplexity2(sentence) \n",
    "    sent = ' '.join(wrd[-1] for wrd in sentence)\n",
    "    test_perflexity = [*dev_perflexity,[pp,sent]]\n",
    "# dev data perplexity\n",
    "train_perflexity =[]\n",
    "ngram_tokenized[:1]\n",
    "for sentence in ngram_tokenized:\n",
    "    pp = perplexity2(sentence) \n",
    "    sent = ' '.join(wrd[-1] for wrd in sentence)\n",
    "    train_perflexity = [*dev_perflexity,[pp,sent]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean perflexity on train data: 851.7242660032439\n",
      "mean perflexity on dev data: 851.778108102394\n",
      "mean perflexity on test data: 851.6859354130299\n"
     ]
    }
   ],
   "source": [
    "print(f\"mean perflexity on train data: {np.asarray(np.array(train_perflexity)[:,0],dtype='float64').mean()}\")\n",
    "\n",
    "print(f\"mean perflexity on dev data: {np.asarray(np.array(dev_perflexity)[:,0],dtype='float64').mean()}\")\n",
    "print(f\"mean perflexity on test data: {np.asarray(np.array(test_perflexity)[:,0],dtype='float64').mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i stuff <unk> <unk> asking <unk> that <unk> <unk>\n"
     ]
    }
   ],
   "source": [
    "test_sentence = test_data[1]\n",
    "s_prob = []\n",
    "index=[]\n",
    "N=len(test_words)\n",
    "for tuples in test_sentence:\n",
    "    pred, prob  = model.predict_nextword(tuples) # get pro\n",
    "    if pred == '<e>':\n",
    "        break\n",
    "    index = index + [pred]\n",
    "    s_prob += [prob]\n",
    "    N +=1\n",
    "print(' '.join(word for word in index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.000269923746541602,\n",
       " 0.00013521736190926916,\n",
       " 6.759040216289287e-05,\n",
       " 6.759040216289287e-05,\n",
       " 0.0001351990806462516,\n",
       " 6.759040216289287e-05,\n",
       " 0.00013520822065981613,\n",
       " 6.759040216289287e-05,\n",
       " 6.759040216289287e-05]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.297428819787847\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10067.576537522376"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity1(s_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.906337915925864"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity2(s_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-e2a96e5f2a70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mprettytable\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPrettyTable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Word'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Screen Name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Hashtag'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhashtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPrettyTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'words' is not defined"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "for label, data in (('Word', words),('Screen Name', screen_names),('Hashtag', hashtags)):\n",
    "\n",
    "    pt = PrettyTable(field_names=[label, 'Count'])    \n",
    "    c = Counter(data)    \n",
    "    [ pt.add_row(kv) for kv in c.most_common()[:10] ]   \n",
    "    pt.align[label], pt.align['Count'] = 'l', 'r'\n",
    "    # Set column alignmentprint(pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - ptable\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ptable-0.9.2               |             py_0          22 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:          22 KB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  ptable             conda-forge/noarch::ptable-0.9.2-py_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "ptable-0.9.2         | 22 KB     | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
